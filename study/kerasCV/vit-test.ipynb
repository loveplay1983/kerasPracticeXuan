{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91856d39-29df-4fc7-b43a-bbd49dedc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b3ea3f-37d4-4452-8ef2-b325dbbfc00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5eed53e-11ac-4784-bed7-aceb463d4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c39228db-10b1-48b2-b9d5-3626cdaaf8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "Current tensorflow -> 2.16.1\n",
      "Current keras -> 3.1.1\n"
     ]
    }
   ],
   "source": [
    "# Test keras version and cuda installation\n",
    "for each in tf.config.list_physical_devices():\n",
    "    print(each)\n",
    "    \n",
    "print(\"Current tensorflow ->\", tf.__version__)\n",
    "print(\"Current keras ->\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "849b3068-14db-484c-9245-59a579e2adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# data preparation\n",
    "\n",
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54701761-80cc-4384-b781-ba17559be969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters configuration\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 10  # actual training with 100, 10 for test\n",
    "\n",
    "image_size = 72  # target image size regards preprocessing\n",
    "patch_size = 6   # image size of each patched content\n",
    "num_patches = (image_size // patch_size) **2   #\n",
    "\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # size of transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024\n",
    "]   # size of the dense layers of the final head (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19254cc4-0fb8-4cd4-9406-c1cb33e92a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9430107-ea50-47ec-945a-dfe6fb179605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fefb5fc2-c3de-4231-bfac-0286c402e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = ops.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = ops.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6515d277-f886-4eb6-925c-6dbc736d7d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling Patches.call().\n\n\u001b[1mcannot compute Conv2D as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Conv2D] name: \u001b[0m\n\nArguments received by Patches.call():\n  â€¢ images=tf.Tensor(shape=(1, 72, 72, 3), dtype=int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m resized_image \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(\n\u001b[1;32m      8\u001b[0m     ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor([image]), \n\u001b[1;32m      9\u001b[0m     size\u001b[38;5;241m=\u001b[39m(image_size, image_size)\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[43mPatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m X \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m X \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Workstation/Anaconda/anaconda/envs/keras3test/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[42], line 14\u001b[0m, in \u001b[0;36mPatches.call\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     12\u001b[0m num_patches_h \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[1;32m     13\u001b[0m num_patches_w \u001b[38;5;241m=\u001b[39m width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[0;32m---> 14\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m patches \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m     16\u001b[0m     patches,\n\u001b[1;32m     17\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     ),\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m patches\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Patches.call().\n\n\u001b[1mcannot compute Conv2D as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Conv2D] name: \u001b[0m\n\nArguments received by Patches.call():\n  â€¢ images=tf.Tensor(shape=(1, 72, 72, 3), dtype=int32)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "resized_image = ops.image.resize(\n",
    "    ops.convert_to_tensor([image]), \n",
    "    size=(image_size, image_size)\n",
    ")\n",
    "\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7c177-e9ca-4c3c-acd3-dcbf15050758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras3",
   "language": "python",
   "name": "keras3test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
