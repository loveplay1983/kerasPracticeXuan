{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0aebd8a8-38f1-4766-80bc-2be91adc627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.applications import efficientnet\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "keras.utils.set_random_seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "72e43af5-c12a-4b61-ad13-9de4cfe32daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "Current tensorflow -> 2.16.2\n",
      "Current keras -> 3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Test keras version and cuda installation\n",
    "for each in tf.config.list_physical_devices():\n",
    "    print(each)\n",
    "    \n",
    "print(\"Current tensorflow ->\", tf.__version__)\n",
    "print(\"Current keras ->\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429ff30-446b-4979-a8ec-9897d6c6d051",
   "metadata": {},
   "source": [
    "```shell\n",
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
    "!unzip -qq Flickr8k_Dataset.zip\n",
    "!unzip -qq Flickr8k_text.zip\n",
    "!rm Flickr8k_Dataset.zip Flickr8k_text.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "103a0bb0-4bcb-484d-86ee-f9ea7982ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(workstation: str) -> str:\n",
    "    if workstation == \"office\":\n",
    "        IMAGE_PATH = \"/media/loveplay1983/data/ML/imgcap/Flickr/flickr8k/Flicker8k_Dataset\"\n",
    "    else: \n",
    "        IMAGE_PATH = \"/media/dllab/AI-PhD-Study/test-dataset/imgcap/Flickr/flickr8k_/Flickr8k_Dataset\"\n",
    "    return IMAGE_PATH\n",
    "\n",
    "def get_cap_path(workstation: str) -> str:\n",
    "    if workstation == \"office\":\n",
    "        CAP_PATH = \"/media/loveplay1983/data/ML/imgcap/Flickr/flickr8k/Flickr8k_text\"\n",
    "    else:\n",
    "        CAP_PATH = \"/media/dllab/AI-PhD-Study/test-dataset/imgcap/Flickr/flickr8k_/Flickr8k_text\"\n",
    "    return CAP_PATH\n",
    "\n",
    "IMAGE_SIZE = (229, 229)\n",
    "VOCAB_SIZE = 10000 # text vocab size\n",
    "SEQ_LENGTH = 25    # fixed length for the text sequence\n",
    "EMBED_DIM = 512 # d_model, feature embedding dimension\n",
    "FF_DIM = 512 # per layer units in the feed forward \n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bf929f7c-5f2b-4a5e-ab6b-4a5965264f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preparation\n",
    "\n",
    "def load_captions_data(filename):\n",
    "    \"\"\"\n",
    "    Loads captions(text) data and maps them to the corresponding images\n",
    "    Args:\n",
    "        filename: Path to the text file containing caption data\n",
    "    Returns: \n",
    "        caption_mapping: Dictionary mapping image names and the corresponding captions\n",
    "        text_data: List containing all the available captions\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename) as caption_file:\n",
    "        caption_data = caption_file.readlines()\n",
    "        caption_mapping={}\n",
    "        text_data=[]\n",
    "        images_to_skip = set()\n",
    "\n",
    "        for line in caption_data:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            img_name, caption = line.split(\"\\t\")  # img name  - captions\n",
    "    \n",
    "            img_name = img_name.split(\"#\")[0]\n",
    "            img_name = os.path.join(get_image_path(\"home\"), img_name.strip())\n",
    "    \n",
    "            tokens = caption.strip().split()\n",
    "    \n",
    "            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n",
    "                \"\"\"\n",
    "                skip image data which are linked to too short or too long caption\n",
    "                \"\"\"\n",
    "                images_to_skip.add(img_name)\n",
    "                continue\n",
    "    \n",
    "            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n",
    "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
    "                text_data.append(caption)\n",
    "    \n",
    "                if img_name in caption_mapping:\n",
    "                    caption_mapping[img_name].append(caption)\n",
    "                else:\n",
    "                    caption_mapping[img_name] = [caption]\n",
    "\n",
    "        for img_name in images_to_skip:\n",
    "            if img_name in caption_mapping:\n",
    "                del caption_mapping[img_name]\n",
    "\n",
    "        return caption_mapping, text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0d0a866c-5335-4320-861a-b3ea3c87b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(caption_data, train_size=.8, shuffle=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        caption_data (dict): Dictionary containing the mapped caption data\n",
    "        train_size (float): Fraction of all the full dataset to use as training data\n",
    "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
    "\n",
    "    Returns:\n",
    "        Traning and validation datasets as two separated dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the list of all images\n",
    "    all_images = list(caption_data.keys())\n",
    "\n",
    "    # shuffle\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "    # data split\n",
    "    train_size = int(len(caption_data) * train_size)\n",
    "\n",
    "    training_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
    "    }\n",
    "\n",
    "    validation_data  = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
    "    }\n",
    "\n",
    "    # return the splits\n",
    "    return training_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0849507d-4145-4e2d-87b5-8f61c141dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/dllab/AI-PhD-Study/test-dataset/imgcap/Flickr/flickr8k_/Flickr8k_text/Flickr8k.token.txt\n",
      "Number of training samples: 6114\n",
      "Number of validation samples: 1529\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "cap_data = os.path.join(get_cap_path(\"home\"), \"Flickr8k.token.txt\")\n",
    "print(cap_data)\n",
    "\n",
    "captions_mapping, text_data = load_captions_data(cap_data)\n",
    "\n",
    "train_data, valid_data = train_val_split(captions_mapping)\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bed1c7cf-7d56-47e1-820c-5cebd5b04e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c3c7bdb-9f12-4524-a6a6-b70d75cec7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b8745759-9411-49dd-8d14-0a819f19e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the text data\n",
    "\n",
    "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "strip_chars = strip_chars.replace(\"<\", \"\")\n",
    "strip_chars = strip_chars.replace(\">\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "685bcdd3-4843-4189-90b7-9798112feb73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = SEQ_LENGTH,\n",
    "    standardize = custom_standardization\n",
    ")\n",
    "\n",
    "vectorization.adapt(text_data)\n",
    "\n",
    "# Data augmentation\n",
    "image_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(.2),\n",
    "        layers.RandomContrast(.3),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eafaf08f-f3f2-4868-8490-b9ee9b2a5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tf.data.Dataset pipeline for training\n",
    "# 1. read the image form the disk\n",
    "# 2. tokenize the five captions corresponding to the image\n",
    "\n",
    "def decode_and_resize(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def process_input(img_path, captions):\n",
    "    return decode_and_resize(img_path), vectorization(captions)\n",
    "\n",
    "def make_dataset(images, captions):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
    "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
    "    dataset = dataset.map(process_input, num_parallel_calls = AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a8102b26-a6ae-43ad-bbeb-ee94680cb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
    "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4a11103a-bff4-4e51-98f1-7458248bf8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.map_op._ParallelMapDataset"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f1e70-59bb-4ab8-96fd-25b760153c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "09e8d7fb-952a-4056-8e81-a28e7822f055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6114, 1529)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365c91b-07ce-4b29-a9fd-4e6b78044049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f4f76-3f01-4624-b2ce-36718a09a720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540de1f2-a1d5-4e73-ac17-181630f55d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867aba5-43f2-401f-b923-b4672b5ee5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff6ccc-f228-40be-a826-995e69b9ef77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fcde4e-5728-4d54-ad6b-1cb30a5704bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e6dd6-1cb2-4b03-9915-efe8a4847733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f907cc-508d-48f5-8e58-da37db5efcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9b735-37cd-48d6-bc94-a72595a2b52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b887c-5f84-4fd2-b742-e072ec38bf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c310a-591a-4315-8b97-06595c4ab26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddee598-5b49-44bc-8f26-f47ec79c5882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9d313-e3db-4cf5-871a-7eb7fff91cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e280b7e-2056-48bd-adbd-29ced5c95e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd666fb-c38a-4a1a-a5a3-35301351c4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800863b-1988-4d09-9fb0-a6c0d0b58a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69487deb-8053-43be-bb17-fc82442d8975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c4ae8-247f-4714-b9b9-5125b2fb7d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8113e62-a8fb-49be-8567-671b8d593fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9583c288-1988-42dd-a278-bc613c7fd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "def get_cnn_model():\n",
    "    base_model = efficientnet.EfficientNetB0(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\"\n",
    "    )\n",
    "    # We freeze our feature extractor\n",
    "    base_model.trainable = False\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b606f0d8-e081-427c-9bb0-9aed5b9a73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, \n",
    "            key_dim=embed_dim,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        inputs = self.layernorm_1(inputs)\n",
    "        inputs = self.dense_1(inputs)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=None,\n",
    "            training=training\n",
    "        )\n",
    "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "        return out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f1e2a6d0-b25a-4b4b-aab2-062140c30ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim = embed_dim,\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim = sequence_length, \n",
    "            output_dim = embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_tokens = embedded_tokens * self.embed_scale\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "183fcb0d-c559-4967-9081-ce9bd45acf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, \n",
    "            key_dim=embed_dim,\n",
    "            dropout=.1,\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            dropout =.1,\n",
    "        )\n",
    "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "        \n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.embedding = PositionalEmbedding(\n",
    "            embed_dim = EMBED_DIM,\n",
    "            sequence_length = SEQ_LENGTH,\n",
    "            vocab_size = VOCAB_SIZE,\n",
    "        )\n",
    "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = layers.Dropout(.3)\n",
    "        self.dropout_2 = layers.Dropout(.5)\n",
    "        self.suports_masking=True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [\n",
    "                tf.expand_dims(batch_size, -1),\n",
    "                tf.constant([1, 1], dtype=tf.int32),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011c57d-cdf3-43b9-9df9-1f08e5ab061b",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_causal_attention_mask(self, inputs):\n",
    "    input_shape = tf.shape(inputs)\n",
    "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "    \n",
    "    # Create a lower triangular matrix of ones\n",
    "    mask = tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)\n",
    "    \n",
    "    # Broadcast the mask to the batch size\n",
    "    mask = tf.broadcast_to(mask[None, None, :, :], (batch_size, 1, sequence_length, sequence_length))\n",
    "    \n",
    "    return mask\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a5d174d-cb52-45a3-b06b-e66cf2b745f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        cnn_model,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        num_captions_per_image=5,\n",
    "        image_aug = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model,\n",
    "        self.encoder = encoder,\n",
    "        self.decoder = decoder,\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "        self.num_captions_per_image = num_captions_per_image\n",
    "        self.image_aug = image_aug\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        # padding loss removal\n",
    "        loss *= mask\n",
    "        # The average loss per non-padding token in the batch.\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        # The accuracy in image captioning is typically computed at the token level, not at the entire caption level.\n",
    "        #  (batch_size, max_sequence_length, vocabulary_size)\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        acuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "        encoder_out = self.encoder(img_embed, training=training)\n",
    "        batch_seq_inp = batch_seq[:, :-1]\n",
    "        batch_seq_true = batch_seq[:, 1:]\n",
    "        mask = tf.match.not_equal(batch_seq_true, 0)\n",
    "        batch_seq_pred = self.decoder(\n",
    "            batch_seq_inp,\n",
    "            encoder_out,\n",
    "            training=training,\n",
    "            mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(batch_seq_true,\n",
    "                                   batch_seq_pred,\n",
    "                                   mask)\n",
    "        acc = self.calculate_accuracy(batch_seq_true, \n",
    "                                      batch_seq_pred, \n",
    "                                      mask)\n",
    "        return loss, acc\n",
    "\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        if self.image_aug:\n",
    "            batch_img = self.image_aug(batch_img)\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, acc = self._compute_caption_loss_and_acc(\n",
    "                    img_embed, batch_seq[:, i, :], training=True\n",
    "                )\n",
    "\n",
    "                # 3. Update loss and accuracy\n",
    "                batch_loss += loss\n",
    "                batch_acc += acc\n",
    "\n",
    "            # 4. Get the list of all the trainable weights\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "            )\n",
    "\n",
    "            # 5. Get the gradients\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "\n",
    "            # 6. Update the trainable weights\n",
    "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "        # 7. Update the trackers\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 8. Return the loss and accuracy values\n",
    "        return {\n",
    "            \"loss\": self.loss_tracker.result(),\n",
    "            \"acc\": self.acc_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            loss, acc = self._compute_caption_loss_and_acc(\n",
    "                img_embed, batch_seq[:, i, :], training=False\n",
    "            )\n",
    "\n",
    "            # 3. Update batch loss and batch accuracy\n",
    "            batch_loss += loss\n",
    "            batch_acc += acc\n",
    "\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "\n",
    "        # 4. Update the trackers\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 5. Return the loss and accuracy values\n",
    "        return {\n",
    "            \"loss\": self.loss_tracker.result(),\n",
    "            \"acc\": self.acc_tracker.result(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker, self.acc_tracker]\n",
    "\n",
    "\n",
    "cnn_model = get_cnn_model()\n",
    "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
    "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_aug=image_augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77619b1c-1aad-4a3d-94fc-3e7fb99fa9f4",
   "metadata": {},
   "source": [
    "1. **`loss = self.loss(y_true, y_pred)`:**\n",
    "   - Calculates the loss between the predicted caption (y_pred) and the ground truth caption (y_true) using the model's defined loss function (self.loss). This is typically a cross-entropy loss for sequence prediction tasks.\n",
    "2. **`mask = tf.cast(mask, dtype=loss.dtype)`:**\n",
    "   - Ensures that the mask has the same data type as the loss for element-wise multiplication.\n",
    "3. **`loss \\*= mask`:**\n",
    "   - Applies the mask to the loss. The mask is used to ignore padding tokens in the caption sequences. By multiplying the loss with the mask, the loss for padding tokens becomes zero, effectively excluding them from the calculation.\n",
    "4. **`return tf.reduce_sum(loss) / tf.reduce_sum(mask)`:**\n",
    "   - Calculates the average loss by summing the masked loss and dividing it by the total number of non-padding tokens (sum of the mask). This gives the average loss per non-padding token in the batch.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- The function calculates the loss for a batch of image-caption pairs.\n",
    "- It incorporates a mask to ignore padding tokens and compute the loss only for actual words.\n",
    "- The final output is the average loss per non-padding token in the batch.\n",
    "\n",
    "___\n",
    "\n",
    "```python\n",
    "return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "```\n",
    "\n",
    ">In essence, the mask acts as a filter, and tf.reduce_sum(mask) provides the count of elements that pass through the filter (non-padding tokens). This count is used to normalize the loss and get a meaningful average loss per non-padding token in the batch\n",
    "\n",
    "___\n",
    "### Why is `axis=2`\n",
    "\n",
    "1. Let's consider a tensor `y_pred` with the shape `(batch_size, max_sequence_length, vocabulary_size)`.\n",
    "   - **batch_size:** This dimension represents the number of image-caption pairs in the batch.\n",
    "     - The first element along this dimension corresponds to the first caption and its associated data.\n",
    "     - The second element corresponds to the second caption, and so on.\n",
    "   - **max_sequence_length:** This dimension represents the maximum number of tokens in a caption.\n",
    "   - **vocabulary_size:** This dimension represents the size of the vocabulary.\n",
    "  \n",
    "___\n",
    "### Why `shifting` \n",
    "```python\n",
    "batch_seq_inp = batch_seq[:, :-1]\n",
    "batch_seq_true = batch_seq[:, 1:]\n",
    "```\n",
    "**Teacher Forcing** The primary reason for shifting is to implement a training technique called \"teacher forcing\". In teacher forcing, instead of feeding the model's previous output as input for the next prediction (like in inference), we directly feed the ground truth token. This stabilizes training and often leads to faster convergence.\n",
    "\n",
    "**Alignment for Loss Calculation** To calculate the loss, we need to compare the predicted tokens with the corresponding ground truth tokens. By shifting `batch_seq` to create `batch_seq_true`, we ensure that the predicted tokens from the decoder align correctly with their respective ground truth tokens.\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "Let's consider a simplified example with a vocabulary size of 5 and a maximum sequence length of 3:\n",
    "\n",
    "```\n",
    "batch_seq = [[1, 2, 3],  # Ground truth caption: \"word1 word2 word3\"\n",
    "            [4, 5, 0]]  # Ground truth caption: \"word4 word5 <pad>\"\n",
    "\n",
    "batch_seq_inp = [[0, 1, 2],  # Input to decoder: <start> word1 word2\n",
    "                [0, 4, 5]]  # Input to decoder: <start> word4 word5\n",
    "\n",
    "batch_seq_true = [[1, 2, 3],  # Target for loss calculation: word1 word2 word3\n",
    "                 [4, 5, 0]]  # Target for loss calculation: word4 word5 <pad>\n",
    "```\n",
    "\n",
    "As you can see, the decoder receives the previous token as input (along with the encoder output) and predicts the next token. The shifted `batch_seq_true` provides the correct target for calculating the loss.\n",
    "\n",
    "**In summary, shifting `batch_seq` is essential for training sequence-to-sequence models like image captioning. It enables teacher forcing and ensures proper alignment for loss calculation.**\n",
    "\n",
    "\n",
    "# important note\n",
    "### Teacher Forcing\n",
    "\n",
    "- **Training phase:** Used to stabilize training and accelerate convergence.\n",
    "- **Process:** The ground truth token is fed as input to the decoder at each time step.\n",
    "\n",
    "### Autoregression\n",
    "\n",
    "- **Inference phase:** Used to generate text sequentially, one token at a time.\n",
    "- **Process:** The model's own previous output is fed as input to predict the next token.\n",
    "\n",
    "**In essence, teacher forcing is a training technique, while autoregression is the fundamental mechanism for generating text sequentially.**\n",
    "\n",
    "### Combining Both\n",
    "\n",
    "It's common to use teacher forcing during training and then switch to autoregressive mode during inference. However, there are also techniques like scheduled sampling that gradually transition from teacher forcing to autoregression during training to improve model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "adfe05a2-b432-43f5-b30e-af93c9b5f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_TupleWrapper' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m caption_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(lr_schedule), loss\u001b[38;5;241m=\u001b[39mcross_entropy)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m caption_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     40\u001b[0m     train_dataset,\n\u001b[1;32m     41\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m     42\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalid_dataset,\n\u001b[1;32m     43\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m~/workstation/AI/utils/Anaconda3/envs/keras3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[94], line 66\u001b[0m, in \u001b[0;36mImageCaptioningModel.train_step\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m     63\u001b[0m     batch_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_aug(batch_img)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 1. Get image embeddings\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m img_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_model(batch_img)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# 2. Pass each of the five captions one by one to the decoder\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# along with the encoder outputs and compute the loss as well as accuracy\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# for each caption.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_captions_per_image):\n",
      "\u001b[0;31mTypeError\u001b[0m: '_TupleWrapper' object is not callable"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    reduction=None,\n",
    ")\n",
    "\n",
    "# EarlyStopping criteria\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler for the optimizer\n",
    "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        global_step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_progress = global_step / warmup_steps\n",
    "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "        return tf.cond(\n",
    "            global_step < warmup_steps,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.post_warmup_learning_rate,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create a learning rate schedule\n",
    "num_train_steps = len(train_dataset) * EPOCHS\n",
    "num_warmup_steps = num_train_steps // 15\n",
    "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
    "\n",
    "# Compile the model\n",
    "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n",
    "\n",
    "# Fit the model\n",
    "caption_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb44460-ccc9-4dd3-a0da-94cbf772f6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e146df1-88a1-4b85-bc18-08744c0b241f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "58da7d61-e986-4342-98e9-fe4e28cdb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    base_model = efficientnet.EfficientNetB0(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "    # We freeze our feature extractor\n",
    "    base_model.trainable = False\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    return cnn_model\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        inputs = self.layernorm_1(inputs)\n",
    "        inputs = self.dense_1(inputs)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "        return out_1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_tokens = embedded_tokens * self.embed_scale\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.embedding = PositionalEmbedding(\n",
    "            embed_dim=EMBED_DIM,\n",
    "            sequence_length=SEQ_LENGTH,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "        )\n",
    "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = layers.Dropout(0.3)\n",
    "        self.dropout_2 = layers.Dropout(0.5)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [\n",
    "                tf.expand_dims(batch_size, -1),\n",
    "                tf.constant([1, 1], dtype=tf.int32),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_model,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        num_captions_per_image=5,\n",
    "        image_aug=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "        self.num_captions_per_image = num_captions_per_image\n",
    "        self.image_aug = image_aug\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "        encoder_out = self.encoder(img_embed, training=training)\n",
    "        batch_seq_inp = batch_seq[:, :-1]\n",
    "        batch_seq_true = batch_seq[:, 1:]\n",
    "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
    "        batch_seq_pred = self.decoder(\n",
    "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        if self.image_aug:\n",
    "            batch_img = self.image_aug(batch_img)\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, acc = self._compute_caption_loss_and_acc(\n",
    "                    img_embed, batch_seq[:, i, :], training=True\n",
    "                )\n",
    "\n",
    "                # 3. Update loss and accuracy\n",
    "                batch_loss += loss\n",
    "                batch_acc += acc\n",
    "\n",
    "            # 4. Get the list of all the trainable weights\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "            )\n",
    "\n",
    "            # 5. Get the gradients\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "\n",
    "            # 6. Update the trainable weights\n",
    "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "        # 7. Update the trackers\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 8. Return the loss and accuracy values\n",
    "        return {\n",
    "            \"loss\": self.loss_tracker.result(),\n",
    "            \"acc\": self.acc_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            loss, acc = self._compute_caption_loss_and_acc(\n",
    "                img_embed, batch_seq[:, i, :], training=False\n",
    "            )\n",
    "\n",
    "            # 3. Update batch loss and batch accuracy\n",
    "            batch_loss += loss\n",
    "            batch_acc += acc\n",
    "\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "\n",
    "        # 4. Update the trackers\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 5. Return the loss and accuracy values\n",
    "        return {\n",
    "            \"loss\": self.loss_tracker.result(),\n",
    "            \"acc\": self.acc_tracker.result(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker, self.acc_tracker]\n",
    "\n",
    "\n",
    "cnn_model = get_cnn_model()\n",
    "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
    "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    image_aug=image_augmentation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8b8fd297-bab1-45c0-aab3-5a2b5196a81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_10\" is incompatible with the layer: expected shape=(None, 229, 229, 3), found shape=(229, 229, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m caption_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(lr_schedule), loss\u001b[38;5;241m=\u001b[39mcross_entropy)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m caption_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     40\u001b[0m     train_dataset,\n\u001b[1;32m     41\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m     42\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalid_dataset,\n\u001b[1;32m     43\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m~/workstation/AI/utils/Anaconda3/envs/keras3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[130], line 204\u001b[0m, in \u001b[0;36mImageCaptioningModel.train_step\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m    201\u001b[0m     batch_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_aug(batch_img)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# 1. Get image embeddings\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m img_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_model(batch_img)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# 2. Pass each of the five captions one by one to the decoder\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# along with the encoder outputs and compute the loss as well as accuracy\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# for each caption.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_captions_per_image):\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional_10\" is incompatible with the layer: expected shape=(None, 229, 229, 3), found shape=(229, 229, 3)"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    reduction=None,\n",
    ")\n",
    "\n",
    "# EarlyStopping criteria\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler for the optimizer\n",
    "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        global_step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_progress = global_step / warmup_steps\n",
    "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "        return tf.cond(\n",
    "            global_step < warmup_steps,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.post_warmup_learning_rate,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create a learning rate schedule\n",
    "num_train_steps = len(train_dataset) * EPOCHS\n",
    "num_warmup_steps = num_train_steps // 15\n",
    "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
    "\n",
    "# Compile the model\n",
    "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n",
    "\n",
    "# Fit the model\n",
    "caption_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f95732-6333-485d-bab6-f60d46e205ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
